{"version":3,"sources":["VideoCamera.js","utils.js","App.js","reportWebVitals.js","index.js"],"names":["VideoCamera","videoRef","useState","cameras","setCameras","activeCamera","setActiveCamera","useEffect","camera","navigator","mediaDevices","enumerateDevices","then","devices","filter","device","kind","catch","error","console","log","getUserMedia","audio","video","facingMode","stream","current","srcObject","setAttribute","play","length","onChange","e","target","value","map","deviceId","label","ref","bodyPartsEmoticons","nose","leftEye","rightEye","drawEmoticons","pose","videoWidth","videoHeight","canvas","keypoints","bodyparts","ctx","getContext","filteredResult","reduce","filtered","option","Object","keys","includes","part","push","emoticon","width","height","minConfidence","scale","font","textAlign","i","keypoint","score","position","y","x","beginPath","arc","Math","PI","fillText","drawTextPoints","App","webcamEl","useRef","canvasEl","detectPose","posenet_model","a","readyState","estimateSinglePose","posenet","inputResolution","model","setInterval","initPoseNet","className","reportWebVitals","onPerfEntry","Function","getCLS","getFID","getFCP","getLCP","getTTFB","ReactDOM","render","StrictMode","document","getElementById"],"mappings":"8XAuDeA,MAnDf,YAAoC,IAAbC,EAAY,EAAZA,SAAY,EACHC,mBAAS,IADN,mBAC1BC,EAD0B,KACjBC,EADiB,OAEOF,mBAAS,IAFhB,mBAE1BG,EAF0B,KAEZC,EAFY,KAiCjC,OAzBAC,qBAAU,WC6BL,IAAwBC,ED3B3BC,UAAUC,aACPC,mBAEAC,MAAK,SAACC,GAAD,OACJA,EAAQC,QAAO,SAACC,GAAD,MAA4B,eAAhBA,EAAOC,WAGnCJ,MAAK,SAACT,GAAD,OAAaC,EAAWD,MAC7Bc,OAAM,SAACC,GACNC,QAAQC,IAAIF,MAGhBT,UAAUC,aACPW,cCcwBb,EDdIH,ECe7BG,EACK,CACLc,OAAO,EACPC,MAAOf,GAIJ,CACLc,OAAO,EACPC,MAAO,CACLC,WAAY,WDxBXZ,MAAK,SAACa,GACL,IAAIF,EAAQtB,EAASyB,QACrBH,EAAMI,UAAYF,EAClBF,EAAMK,aAAa,eAAe,GAClCL,EAAMK,aAAa,YAAY,GAC/BL,EAAMM,YAET,CAAC5B,EAAUI,IAGZ,qCACE,+BACGF,EAAQ2B,QACP,wBAAQC,SAjCW,SAACC,GAC1B1B,EAAgB0B,EAAEC,OAAOC,QAgCnB,SACG/B,EAAQgC,KAAI,SAAC3B,GAAD,OACX,wBAA8B0B,MAAO1B,EAAO4B,SAA5C,SACG5B,EAAO6B,OADG7B,EAAO4B,iBAO5B,uBAAOE,IAAKrC,QEzCZsC,EAAqB,CACzBC,KAAM,eACNC,QAAS,eACTC,SAAU,gBAiBNC,EAAgB,SAACC,EAAMrB,EAAOsB,EAAYC,EAAaC,GAC3D,IDJmCC,EAAWC,ECIxCC,EAAMH,EAAOrB,QAAQyB,WAAW,MAChCC,GDL6BJ,ECMjCJ,EAAKI,UDNuCC,ECO5CV,EDNFS,EAAUK,QAAO,SAACC,EAAUC,GAO1B,OANIC,OAAOC,KAAKR,GAAWS,SAASH,EAAOI,OACzCL,EAASM,KAAT,2BACKL,GADL,IAEEM,SAAUZ,EAAUM,EAAOI,SAGxBL,IACN,KCCHP,EAAOrB,QAAQoC,MAAQjB,EACvBE,EAAOrB,QAAQqC,OAASjB,EDlCnB,SAAwBE,EAAWgB,EAAed,GAAiB,IAAZe,EAAW,uDAAH,EACpEf,EAAIgB,KAAO,aACXhB,EAAIiB,UAAY,SAEhB,IAAK,IAAIC,EAAI,EAAGA,EAAIpB,EAAUlB,OAAQsC,IAAK,CACzC,IAAMC,EAAWrB,EAAUoB,GAE3B,KAAIC,EAASC,MAAQN,GAArB,CAHyC,MAOxBK,EAASE,SAAlBC,EAPiC,EAOjCA,EAAGC,EAP8B,EAO9BA,EAGXvB,EAAIwB,YACJxB,EAAIyB,IAAIF,EAAIR,EAAOO,EAAIP,EAAO,EAAG,EAAG,EAAIW,KAAKC,IAC7C3B,EAAI4B,SAAST,EAASR,SAAUY,EAAGD,KCoBrCO,CAAe3B,EAAgB,GAAKF,EAAK,IA2C5B8B,MAxCf,WACE,IAAMC,EAAWC,iBAAO,MAClBC,EAAWD,iBAAO,MAElBE,EAAU,uCAAG,WAAOC,GAAP,qBAAAC,EAAA,yDACQ,OAArBL,EAASvD,SAAoD,IAAhCuD,EAASvD,QAAQ6D,WADjC,wBAET/E,EAASyE,EAASvD,QAClBmB,EAAaoC,EAASvD,QAAQmB,WAC9BC,EAAcmC,EAASvD,QAAQoB,YAErCmC,EAASvD,QAAQoC,MAAQjB,EACzBoC,EAASvD,QAAQqC,OAASjB,EAPX,SAUOuC,EAAcG,mBAAmBhF,GAVxC,OAUTiF,EAVS,OAWf9C,EAAc8C,EAASjF,EAAQqC,EAAYC,EAAaqC,GAXzC,4CAAH,sDA4BhB,OAbiB,uCAAG,4BAAAG,EAAA,sEACEG,IAAa,CAC/BC,gBAAiB,CAAE5B,MAAO,IAAKC,OAAQ,KACvCE,MAAO,KAHS,OACZ0B,EADY,OAMlBC,aAAY,WACVR,EAAWO,KACV,KARe,2CAAH,oDAWjBE,GAGE,sBAAKC,UAAU,MAAf,UACE,cAAC,EAAD,CAAa7F,SAAUgF,IACvB,wBAAQ3C,IAAK6C,QCjEJY,EAZS,SAAAC,GAClBA,GAAeA,aAAuBC,UACxC,8BAAqBrF,MAAK,YAAkD,IAA/CsF,EAA8C,EAA9CA,OAAQC,EAAsC,EAAtCA,OAAQC,EAA8B,EAA9BA,OAAQC,EAAsB,EAAtBA,OAAQC,EAAc,EAAdA,QAC3DJ,EAAOF,GACPG,EAAOH,GACPI,EAAOJ,GACPK,EAAOL,GACPM,EAAQN,OCDdO,IAASC,OACP,cAAC,IAAMC,WAAP,UACE,cAAC,EAAD,MAEFC,SAASC,eAAe,SAM1BZ,M","file":"static/js/main.28a89b20.chunk.js","sourcesContent":["import { useEffect, useState } from \"react\";\n\nimport { getConstraints } from \"./utils\";\n\nfunction VideoCamera({ videoRef }) {\n  const [cameras, setCameras] = useState([]);\n  const [activeCamera, setActiveCamera] = useState(\"\");\n\n  const handleCameraChange = (e) => {\n    setActiveCamera(e.target.value);\n  };\n\n  useEffect(() => {\n    // Gets all available devices\n    navigator.mediaDevices\n      .enumerateDevices()\n      // Filter devices on video input only\n      .then((devices) =>\n        devices.filter((device) => device.kind === \"videoinput\")\n      )\n      // Store available cameras in state\n      .then((cameras) => setCameras(cameras))\n      .catch((error) => {\n        console.log(error);\n      });\n\n    navigator.mediaDevices\n      .getUserMedia(getConstraints(activeCamera))\n      .then((stream) => {\n        let video = videoRef.current;\n        video.srcObject = stream;\n        video.setAttribute(\"playsinline\", true);\n        video.setAttribute(\"autoplay\", true);\n        video.play();\n      });\n  }, [videoRef, activeCamera]);\n\n  return (\n    <>\n      <form>\n        {cameras.length && (\n          <select onChange={handleCameraChange}>\n            {cameras.map((camera) => (\n              <option key={camera.deviceId} value={camera.deviceId}>\n                {camera.label}\n              </option>\n            ))}\n          </select>\n        )}\n      </form>\n      <video ref={videoRef} />\n    </>\n  );\n}\n\nexport default VideoCamera;\n","/**\n * Draw pose keypoints onto a canvas\n */\nexport function drawTextPoints(keypoints, minConfidence, ctx, scale = 1) {\n  ctx.font = \"16px Arial\";\n  ctx.textAlign = \"center\";\n\n  for (let i = 0; i < keypoints.length; i++) {\n    const keypoint = keypoints[i];\n\n    if (keypoint.score < minConfidence) {\n      continue;\n    }\n\n    const { y, x } = keypoint.position;\n\n    // Draws the text/emoticons on the keypoints\n    ctx.beginPath();\n    ctx.arc(x * scale, y * scale, 3, 0, 2 * Math.PI);\n    ctx.fillText(keypoint.emoticon, x, y);\n  }\n}\n\n/**\n * Get filtered bodyparts\n */\nexport const getFilteredBodyparts = (keypoints, bodyparts) =>\n  keypoints.reduce((filtered, option) => {\n    if (Object.keys(bodyparts).includes(option.part)) {\n      filtered.push({\n        ...option,\n        emoticon: bodyparts[option.part],\n      });\n    }\n    return filtered;\n  }, []);\n\n/**\n * Sets active camera contraints\n * @todo create seperate fuctions for updating camera and getting contraints\n */\nexport function getConstraints(camera) {\n  if (camera) {\n    return {\n      audio: false,\n      video: camera,\n    };\n  }\n\n  return {\n    audio: false,\n    video: {\n      facingMode: \"user\",\n    },\n  };\n}\n","import { useRef } from \"react\";\nimport * as tf from \"@tensorflow/tfjs\";\nimport * as posenet from \"@tensorflow-models/posenet\";\nimport { getFilteredBodyparts, drawTextPoints } from \"./utils\";\n\nimport \"./App.css\";\nimport VideoCamera from \"./VideoCamera\";\n\n// Settings for emoticons\nconst bodyPartsEmoticons = {\n  nose: \"ðŸ‘€\",\n  leftEye: \"ðŸ’©\",\n  rightEye: \"ðŸ’©\",\n  // leftEar: \"\",\n  // rightEar: \"\",\n  // leftShoulder: \"\",\n  // rightShoulder: \"\",\n  // leftElbow: \"\",\n  // rightElbow: \"\",\n  // leftWrist: \"\",\n  // rightWrist: \"\",\n  // leftHip: \"\",\n  // rightHip: \"\",\n  // leftKnee: \"\",\n  // rightKnee: \"\",\n  // leftAnkle: \"\",\n  // rightAnkle: \"\",\n};\n\nconst drawEmoticons = (pose, video, videoWidth, videoHeight, canvas) => {\n  const ctx = canvas.current.getContext(\"2d\");\n  const filteredResult = getFilteredBodyparts(\n    pose.keypoints,\n    bodyPartsEmoticons\n  );\n\n  canvas.current.width = videoWidth;\n  canvas.current.height = videoHeight;\n\n  drawTextPoints(filteredResult, 0.9, ctx, 1);\n};\n\nfunction App() {\n  const webcamEl = useRef(null);\n  const canvasEl = useRef(null);\n\n  const detectPose = async (posenet_model) => {\n    if (webcamEl.current !== null && webcamEl.current.readyState === 4) {\n      const camera = webcamEl.current;\n      const videoWidth = webcamEl.current.videoWidth;\n      const videoHeight = webcamEl.current.videoHeight;\n\n      webcamEl.current.width = videoWidth;\n      webcamEl.current.height = videoHeight;\n\n      // Get pose and detect body parts using the posenet model\n      const posenet = await posenet_model.estimateSinglePose(camera);\n      drawEmoticons(posenet, camera, videoWidth, videoHeight, canvasEl);\n    }\n  };\n\n  const initPoseNet = async () => {\n    const model = await posenet.load({\n      inputResolution: { width: 640, height: 480 },\n      scale: 0.8,\n    });\n\n    setInterval(() => {\n      detectPose(model);\n    }, 5000);\n  };\n\n  initPoseNet();\n\n  return (\n    <div className=\"App\">\n      <VideoCamera videoRef={webcamEl} />\n      <canvas ref={canvasEl} />\n    </div>\n  );\n}\n\nexport default App;\n","const reportWebVitals = onPerfEntry => {\n  if (onPerfEntry && onPerfEntry instanceof Function) {\n    import('web-vitals').then(({ getCLS, getFID, getFCP, getLCP, getTTFB }) => {\n      getCLS(onPerfEntry);\n      getFID(onPerfEntry);\n      getFCP(onPerfEntry);\n      getLCP(onPerfEntry);\n      getTTFB(onPerfEntry);\n    });\n  }\n};\n\nexport default reportWebVitals;\n","import React from 'react';\nimport ReactDOM from 'react-dom';\nimport './index.css';\nimport App from './App';\nimport reportWebVitals from './reportWebVitals';\n\nReactDOM.render(\n  <React.StrictMode>\n    <App />\n  </React.StrictMode>,\n  document.getElementById('root')\n);\n\n// If you want to start measuring performance in your app, pass a function\n// to log results (for example: reportWebVitals(console.log))\n// or send to an analytics endpoint. Learn more: https://bit.ly/CRA-vitals\nreportWebVitals();\n"],"sourceRoot":""}